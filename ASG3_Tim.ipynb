{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UP596VYJFBl0"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "from gensim.models import Word2Vec\n",
        "from gensim import similarities\n",
        "from enum import Enum\n",
        "import json\n",
        "from dataclasses import dataclass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GOOGLE_MODEL = \"word2vec-google-news-300\"\n",
        "\n",
        "models = {GOOGLE_MODEL: api.load(GOOGLE_MODEL)}\n"
      ],
      "metadata": {
        "id": "dHz2ybGXFcsS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Label(Enum):\n",
        "  GUESS = \"guess\"\n",
        "  CORRECT = \"correct\"\n",
        "  WRONG = \"wrong\"\n",
        "\n",
        "@dataclass\n",
        "class Question:\n",
        "  question: str\n",
        "  answer: str\n",
        "  choices: [str]\n",
        "  label: Label = Label.GUESS\n",
        "  guessed_word: str = None"
      ],
      "metadata": {
        "id": "aHj0jJaKXcev"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_simmilar_word(question: Question) -> Question:\n",
        "\n",
        "  try:\n",
        "    suggestions = model.most_similar_cosmul(positive=question.question)\n",
        "  except(KeyError):\n",
        "    question.guessed_word=\"\"\n",
        "    return question\n",
        "\n",
        "  for suggestion, _ in suggestions:\n",
        "    if suggestion in question.choices:\n",
        "      question.guessed_word = suggestion\n",
        "      if question.guessed_word == question.answer: question.label = Label.CORRECT\n",
        "      else: question.label = Label.WRONG\n",
        "      break\n",
        "\n",
        "  return question\n",
        "\n",
        "def predict_most_simmilar_word(question: Question, model=GOOGLE_MODEL) -> Question:\n",
        "  model = models[model]\n",
        "  highest_similarity = (0, \"\")\n",
        "  for choice in question.choices:\n",
        "      try:\n",
        "        similarity = model.similarity(question.question, choice)\n",
        "      except(KeyError):\n",
        "        continue\n",
        "      if similarity > highest_similarity[0]:\n",
        "        highest_similarity = (similarity, choice)\n",
        "\n",
        "  question.guessed_word = highest_similarity[1]\n",
        "  if highest_similarity[0] == 0: return question\n",
        "  if question.answer == question.guessed_word: question.label = Label.CORRECT\n",
        "  else: question.label = Label.WRONG\n",
        "  return question\n",
        "\n",
        "def get_simmilar_list(questions: dict, model=GOOGLE_MODEL) -> list[Question]:\n",
        "  for question in questions:\n",
        "    predict_most_simmilar_word(question, model)\n",
        "\n",
        "  return questions\n",
        "\n"
      ],
      "metadata": {
        "id": "9Sm2j6t3IZiu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown  https://drive.google.com/uc?id=1LAclVWP_FJBhysLAR0ky1wIP3RveORXO\n",
        "\n",
        "INP_FILENAME = \"/content/synonym.json\"\n",
        "EVAL_FILENAME = \"analysis.csv\"\n",
        "\n",
        "def analyze_model(model = GOOGLE_MODEL):\n",
        "  OUTP_FILENAME = f\"{model}-details.csv\"\n",
        "\n",
        "  questions = []\n",
        "  with open(INP_FILENAME) as f:\n",
        "    data = json.load(f)\n",
        "    for obj in data:\n",
        "      questions.append(Question(**obj))\n",
        "\n",
        "  res = get_simmilar_list(questions, model)\n",
        "  with open(OUTP_FILENAME, \"a\") as f:\n",
        "    for question in res:\n",
        "      if question.guessed_word == None:\n",
        "        question.guessed_word = \"\"\n",
        "      f.write(\",\".join([question.question,\n",
        "                        question.answer,\n",
        "                        question.guessed_word,\n",
        "                        question.label.value]))\n",
        "      f.write(\"\\n\")\n",
        "\n",
        "  with open(EVAL_FILENAME, \"a\") as f:\n",
        "    length = len(models[model])\n",
        "    C = len([x for x in res if x.label == Label.CORRECT])\n",
        "    guesses = len([x for x in res if x.label == Label.GUESS])\n",
        "    V = len(res) - guesses\n",
        "    if V != 0:\n",
        "      accuracy = C/V\n",
        "    else: accuracy = 0\n",
        "\n",
        "    f.write(\",\".join([model,str(length), str(C), str(V), str(accuracy)]))\n",
        "    f.write(\"\\n\")"
      ],
      "metadata": {
        "id": "PSlW18RYUrT1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#analyze_model()"
      ],
      "metadata": {
        "id": "JKM3M0fti5Ee"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "from huggingface_hub import hf_hub_download\n",
        "WIKI2018_MODEL = \"wiki2018\"\n",
        "models[WIKI2018_MODEL] = KeyedVectors.load_word2vec_format(hf_hub_download(repo_id=\"Word2vec/wikipedia2vec_enwiki_20180420_100d\", filename=\"enwiki_20180420_100d.txt\"))"
      ],
      "metadata": {
        "id": "VMYT3gJtkLWk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "17e18154-6ff2-4e02-8fc8-ba8a11fa38fe"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-1812140ddbee>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhuggingface_hub\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhf_hub_download\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mWIKI2018_MODEL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"wiki2018\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mWIKI2018_MODEL\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhf_hub_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Word2vec/wikipedia2vec_enwiki_20180420_100d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"enwiki_20180420_100d.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[1;32m   1717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m         \"\"\"\n\u001b[0;32m-> 1719\u001b[0;31m         return _load_word2vec_format(\n\u001b[0m\u001b[1;32m   1720\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1721\u001b[0m             \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_header\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mno_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             )\n\u001b[1;32m   2068\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2069\u001b[0;31m             \u001b[0m_word2vec_read_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2070\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2071\u001b[0m         logger.info(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m_word2vec_read_text\u001b[0;34m(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, encoding)\u001b[0m\n\u001b[1;32m   1973\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unexpected end of input; is count incorrect or file otherwise damaged?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1974\u001b[0m         \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_word2vec_line_to_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1975\u001b[0;31m         \u001b[0m_add_word_to_kv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1976\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m_add_word_to_kv\u001b[0;34m(kv, counts, word, weights, vocab_size)\u001b[0m\n\u001b[1;32m   1906\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_add_word_to_kv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1907\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1908\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mkv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_index_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1909\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"duplicate word '%s' in word2vec file, ignoring all but first\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1910\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "WIKI_GIGIWORLD_MODEL = \"glove-wiki-gigaword-100\"\n",
        "models[WIKI_GIGIWORLD_MODEL] = api.load(WIKI_GIGIWORLD_MODEL)"
      ],
      "metadata": {
        "id": "Ew8vAYQgvzBs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "78379063-aa80-4a5d-b443-b65a04e6940b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-39dc8ce75a8c>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mWIKI_GIGIWORLD_MODEL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"glove-wiki-gigaword-100\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mWIKI_GIGIWORLD_MODEL\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWIKI_GIGIWORLD_MODEL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/downloader.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, return_path)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBASE_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__import__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/gensim-data/glove-wiki-gigaword-100/__init__.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'glove-wiki-gigaword-100'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'glove-wiki-gigaword-100.gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[1;32m   1717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m         \"\"\"\n\u001b[0;32m-> 1719\u001b[0;31m         return _load_word2vec_format(\n\u001b[0m\u001b[1;32m   1720\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1721\u001b[0m             \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_header\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mno_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             )\n\u001b[1;32m   2068\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2069\u001b[0;31m             \u001b[0m_word2vec_read_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2070\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2071\u001b[0m         logger.info(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m_word2vec_read_text\u001b[0;34m(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, encoding)\u001b[0m\n\u001b[1;32m   1969\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_word2vec_read_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1970\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline_no\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1971\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1972\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1973\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unexpected end of input; is count incorrect or file otherwise damaged?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/gzip.py\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_not_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TWITTER_25_MODEL = 'glove-twitter-25'\n",
        "TWITTER_50_MODEL = 'glove-twitter-50'\n",
        "models[TWITTER_25_MODEL] = api.load(TWITTER_25_MODEL)\n",
        "models[TWITTER_50_MODEL] = api.load(TWITTER_50_MODEL)"
      ],
      "metadata": {
        "id": "M-qSgVr8jPli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for model_name in models.keys():\n",
        "#  analyze_model(model_name)"
      ],
      "metadata": {
        "id": "ehAovXQuji6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 3"
      ],
      "metadata": {
        "id": "AbyttgFDotZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.corpus import brown\n",
        "import urllib.request\n",
        "from urllib.error import HTTPError\n",
        "\n",
        "\n",
        "\n",
        "def sentenceise_book(book_file_name: str) -> list[str]:\n",
        "  inp = \"\"\n",
        "  with open(f\"/content/books/{book_file_name}\") as f:\n",
        "    inp = f.read()\n",
        "  res = sent_tokenize(inp)\n",
        "  return res\n",
        "\n",
        "def preprocess_string (inp: str) -> str:\n",
        "  inp = inp.replace(\"\\n\",\" \")\n",
        "  inp = inp.replace(\"\\r\",\" \")\n",
        "  inp = inp.replace(\"\\ufeff\",\" \")\n",
        "  translator = str.maketrans(\"\", \"\", string.punctuation)\n",
        "  inp = inp.translate(translator)\n",
        "  words = inp.split(\" \")\n",
        "  processed_words = []\n",
        "  for word in words:\n",
        "    if len(word) == 0: continue\n",
        "    new_word = word.lower()\n",
        "    processed_words.append(new_word)\n",
        "  return processed_words\n",
        "\n",
        "def preprocess(inp: list[str]):\n",
        "  res = []\n",
        "  for sentence in inp:\n",
        "    ps = preprocess_string(sentence)\n",
        "    if len(ps) > 0: res.append(ps)\n",
        "  return res\n",
        "\n",
        "\n",
        "def train_model(inp: list[list[str]], window_size:int, embedding_size:int) -> KeyedVectors:\n",
        "  model = Word2Vec(\n",
        "    sentences=inp,\n",
        "    window=window_size,\n",
        "    vector_size=embedding_size\n",
        "    )\n",
        "  return model.wv\n",
        "\n",
        "def create_wordList_from_folder(folder: str):\n",
        "  onlyfiles = [f for f in listdir(folder) if isfile(join(folder, f))]\n",
        "  res = []\n",
        "  for file in onlyfiles:\n",
        "    raw_book = sentenceise_book(file)\n",
        "    sentences = preprocess(raw_book)\n",
        "    res.extend(sentences)\n",
        "  return res\n",
        "\n",
        "def create_wordList_from_gutenberg(max_books: int):\n",
        "  res = []\n",
        "  for i in range(1,max_books):\n",
        "    try:\n",
        "      print(f\"downloading book {i}\")\n",
        "      raw_data = urllib.request.urlopen(f\"https://www.gutenberg.org/cache/epub/{i}/pg{i}.txt\")\n",
        "      raw_data = [line.decode('utf-8') for line in raw_data]\n",
        "      print(f\"creating sentences for {i}\")\n",
        "      sentences = preprocess(raw_data)\n",
        "      res.extend(sentences)\n",
        "    except HTTPError:\n",
        "      print(f\"{i} could not be dowloaded\")\n",
        "  print(\"wordlist finsihed\")\n",
        "  return res\n"
      ],
      "metadata": {
        "id": "RqIYr2r5oq1p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6ac2a8d-f461-40c6-9bf1-8bcf15ecd27e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = urllib.request.urlopen(\"https://www.gutenberg.org/cache/epub/72294/pg72294.txt\")\n",
        "\n",
        "\n",
        "print([line.decode('utf-8') for line in data])"
      ],
      "metadata": {
        "id": "52H4jnkXD95j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "BOOK_FOLDER = \"/content/books/\"\n",
        "CUSTOM_MODEL = \"team_J_model\"\n",
        "BOOK_COUNT = 50\n",
        "\n",
        "EMBEDDING_SIZES = [10,20]\n",
        "WINDOW_SIZES = [3, 5]\n",
        "\n",
        "for embedding in EMBEDDING_SIZES:\n",
        "  for window in WINDOW_SIZES:\n",
        "    l = create_wordList_from_gutenberg(BOOK_COUNT)\n",
        "    print(\"training model\")\n",
        "    models[f\"{CUSTOM_MODEL}_E{embedding}_W{window}\"] = train_model(l)"
      ],
      "metadata": {
        "id": "wfm4kn8V8Pyi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "744558e1-e0ec-4e87-f239-9b08be6ac595"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading book 1\n",
            "creating sentences for 1\n",
            "downloading book 2\n",
            "creating sentences for 2\n",
            "downloading book 3\n",
            "creating sentences for 3\n",
            "downloading book 4\n",
            "creating sentences for 4\n",
            "downloading book 5\n",
            "creating sentences for 5\n",
            "downloading book 6\n",
            "creating sentences for 6\n",
            "downloading book 7\n",
            "creating sentences for 7\n",
            "downloading book 8\n",
            "creating sentences for 8\n",
            "downloading book 9\n",
            "creating sentences for 9\n",
            "downloading book 10\n",
            "creating sentences for 10\n",
            "downloading book 11\n",
            "creating sentences for 11\n",
            "downloading book 12\n",
            "creating sentences for 12\n",
            "downloading book 13\n",
            "creating sentences for 13\n",
            "downloading book 14\n",
            "creating sentences for 14\n",
            "downloading book 15\n",
            "creating sentences for 15\n",
            "downloading book 16\n",
            "creating sentences for 16\n",
            "downloading book 17\n",
            "creating sentences for 17\n",
            "downloading book 18\n",
            "creating sentences for 18\n",
            "downloading book 19\n",
            "creating sentences for 19\n",
            "downloading book 20\n",
            "creating sentences for 20\n",
            "downloading book 21\n",
            "creating sentences for 21\n",
            "downloading book 22\n",
            "creating sentences for 22\n",
            "downloading book 23\n",
            "creating sentences for 23\n",
            "downloading book 24\n",
            "creating sentences for 24\n",
            "downloading book 25\n",
            "creating sentences for 25\n",
            "downloading book 26\n",
            "creating sentences for 26\n",
            "downloading book 27\n",
            "creating sentences for 27\n",
            "downloading book 28\n",
            "creating sentences for 28\n",
            "downloading book 29\n",
            "creating sentences for 29\n",
            "downloading book 30\n",
            "creating sentences for 30\n",
            "downloading book 31\n",
            "creating sentences for 31\n",
            "downloading book 32\n",
            "creating sentences for 32\n",
            "downloading book 33\n",
            "creating sentences for 33\n",
            "downloading book 34\n",
            "creating sentences for 34\n",
            "downloading book 35\n",
            "creating sentences for 35\n",
            "downloading book 36\n",
            "creating sentences for 36\n",
            "downloading book 37\n",
            "creating sentences for 37\n",
            "downloading book 38\n",
            "creating sentences for 38\n",
            "downloading book 39\n",
            "creating sentences for 39\n",
            "downloading book 40\n",
            "40 could not be dowloaded\n",
            "downloading book 41\n",
            "creating sentences for 41\n",
            "downloading book 42\n",
            "creating sentences for 42\n",
            "downloading book 43\n",
            "creating sentences for 43\n",
            "downloading book 44\n",
            "creating sentences for 44\n",
            "downloading book 45\n",
            "creating sentences for 45\n",
            "downloading book 46\n",
            "creating sentences for 46\n",
            "downloading book 47\n",
            "creating sentences for 47\n",
            "downloading book 48\n",
            "creating sentences for 48\n",
            "downloading book 49\n",
            "creating sentences for 49\n",
            "wordlist finsihed\n",
            "training model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for model_name in models.keys():\n",
        "  analyze_model(model_name)"
      ],
      "metadata": {
        "id": "Kj75dq0NJr8c"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.decomposition import PCA\n",
        "from matplotlib import pyplot\n",
        "\n",
        "nltk.download(\"brown\")\n",
        "\n",
        "# Preprocessing data to lowercase all words and remove single punctuation words\n",
        "document = brown.sents()\n",
        "\n",
        "print(document[0:10])"
      ],
      "metadata": {
        "id": "maMvpY74ztje"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}